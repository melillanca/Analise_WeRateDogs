{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "undefined-valve",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "This document details the efforts needed to carry out the data wrangling work in this project.\n",
    "\n",
    "The data wrangling process consists of three main tasks: Gather, Assess and Clean. This process has also been followed in this project.\n",
    "\n",
    "## Gather\n",
    "\n",
    "Three files need to be gathered. One comes from Udacity who have an archive of tweets from the WeRateDogs account, however the information attached to each tweet is incomplete. The second is another file provided by Udacity with the results of the work of an image recognition algorithm, which identifies the dog breeds contained in the images in each of the tweets. The third is a file generated in this project, from the interaction with the Twitter API; the ID of each tweet is sent and the API returns the complete information of each one of them.\n",
    "\n",
    "In order to gather the data sources, it was first necessary to check what type of source they were. While they all correspond to text files, the format of this text is different. The first file corresponds to a CSV, the second to a TSV and the third is a JSON (the format in which the Twitter API delivers the information). It should be noted that for each file, a Pandas Dataframe was generated.\n",
    "\n",
    "For the CSV file, it was only necessary to apply the pandas library method, read_csv(). \n",
    "For the TSV file, the requests library was used and from there, the get method was used with the corresponding url to obtain the file. Then, Python methods are used to dump this response into a text file.\n",
    "To generate the JSON, it was necessary to register as a developer on Twitter, to obtain access keys such as consumer, token and so on. Subsequently, a text file is generated to store the information to be received and a loop is generated going through the dataset generated from the CSV; the ID of each tweet is extracted and queried through the API. It is necessary to consider a time of 20 to 30 minutes due to the limitations placed by Twitter so as not to saturate its network.\n",
    "\n",
    "These dataframes are joined together to generate a single dataframe.\n",
    "\n",
    "## Assess\n",
    "\n",
    "This dataframe is evaluated, finding the following anomalies.\n",
    "\n",
    "<ul>\n",
    "    <li>There are three columns with non-descriptve names.</li>\n",
    "    <li>There are tweets, which correspond to retweets (values of 'retweeted_status_id' different to 'NaN'), although this is not an inaccuracy or a wrong data in itself, given the requirement for this project not to consider retweets, it is also something that needs to be improved.</li>\n",
    "    <li>There are few missing data. Strictly speaking, there are NaN values and others not reported, but given the context it is accepted that this is the case. Some examples are found in the columns 'in_reply_to...' and 'retweeted_status_...'; in these cases the existence of NaN values indicates that it does not correspond to a reply to another tweet or that it does not correspond to a retweet, respectively.</li>\n",
    "    <li>There are unreported values for 'expanded_urls'</li>\n",
    "    <li> For the sizes ('doggo','floofer','pupper','puppo'), some rows from the file with the dog breed predictions are not informed, which is due to the fact that not all tweets obtained from the API have a correspondence in the file with the breed predictions.</li>\n",
    "    <li>Dog sizes are in separate columns, not complying with the tidiness principle of \"Each variable forms a column\", it should be a single variable, these columns are untidy.</li>\n",
    "    <li>Some strange names for dog breeds are observed, such as 'orange', 'paper_towel', 'basset', among others. However, the breed predictions are marked with the result of the test to the prediction, through a boolean value.</li>\n",
    "    <li>The column 'expanded_urls' in some cases consists of several URLs, so the condition \"Each variable forms a column\" is not fulfilled.</li>\n",
    "    <li>There are evident outliers for the numerator and denominator of the rating.</li>\n",
    "    <li>The 'source' format is not very readable, as it comes inside html tags.</li>\n",
    "</ul>\n",
    "\n",
    "## Clean\n",
    "These anomalies are finally resolved one by one until a clean and tidy dataset is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-reverse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
